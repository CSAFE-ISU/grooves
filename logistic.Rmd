---
title: "Logistic Regression"
output: pdf_document
---

## Introduction  

We are interested in trying a logistic regression approach to predicting which data points are part of the land engraved area (LEA), and which are part of the groove engraved area (GEA) in our 3D bullet land scans.  

This is primarily a two-class classification problem. We will begin with logistic regression, and move to more sophisticated data processing or modeling as needed.  


```{r packages-and-data, echo = F, message = F, warning = F}
#install.packages("tidyr")
#install.packages("dplyr")
#install.packages("purrr")
#install.packages("ggplot2")
library(tidyr)
library(plyr)
library(dplyr)
library(purrr)
library(ggplot2)

#devtools::install_github("CSAFE-ISU/bulletr")
#library(bulletr)

hamby44 <- readRDS("//opt/hamby44/hamby44.rda")
hamby44[1:6,] # to look at what the first 6 rows look like.  
```

```{r rm-empty-lands, echo = F, message = F, warning = F}
hamby44 <- hamby44 %>% filter(!is.na(crosscuts))
```

```{r avg-and-std-values, echo = F, message = F, warning = F}

hamby44 <- hamby44 %>% mutate(ccdata_avg = purrr::map(ccdata, .f = function(dframe){
  dframe <- dframe %>% group_by(y) %>% dplyr::summarise(value = mean(value, na.rm = T))
  dframe <- as.data.frame(dframe)
  check_min <- min(dframe$value[!is.na(dframe$value)])
  dframe <- dframe %>% mutate(value_std = value - check_min)
  return(dframe)
}))

#hamby44

#bullet <- hamby44$ccdata_avg[[1]] ## can be used to visualize the shift down to "baseline" 
#ggplot() + geom_point(data = bullet, aes(x = y, y = value)) + 
#  geom_line(data = bullet, aes(x = y, y = value), color = "red") + 
#  geom_line(data = bullet, aes(x = y, y = value_std), colour = "green") + 
#  theme_bw()
```

```{r calculate-residuals-rlm-rloess, echo = F, warning = F, message = F}
#install.packages("locfit")
library(locfit)
hamby44 <- hamby44 %>% mutate(ccdata_w_resid = purrr::map(ccdata_avg, .f = function(bullet){
  lm0 <- MASS::rlm(value_std~poly(y,2), data=bullet, maxit=100)
  bullet$pred <- predict(lm0, newdata=bullet)

  bullet$absresid <- with(bullet, abs(value_std-pred))
  bullet$resid <- with(bullet, value_std-pred)
  
  
  robust_loess_fit <- locfit.robust(value_std~y, data = bullet, alpha = 1, kern = "tcub")
  bullet$rlo_pred <- predict(robust_loess_fit, newdata = bullet)
  
  bullet$rlo_absresid <- with(bullet, abs(value_std-rlo_pred))
  bullet$rlo_resid <- with(bullet, value_std-rlo_pred)
  return(bullet)
}), rlm_r2 = purrr::map_dbl(ccdata_w_resid, .f = function(bullet){
  r2 <- (cor(bullet$value_std, bullet$pred, use = "complete.obs"))^2
  return(r2)
}), rlo_r2 = purrr::map_dbl(ccdata_w_resid, .f = function(bullet){
  r2 <- (cor(bullet$value_std, bullet$rlo_pred, use = "complete.obs"))^2
  return(r2)
})
)

# to plot any of the individual lands (indexed by i), and what the robust loess predictions look like on that land.  

#bullet_plot <- function(i){
#  bullet <- hamby44$ccdata_w_resid[[i]]
#  bullet %>% ggplot() + geom_point(aes(x = y, y = value_std)) + theme_bw() + geom_line(aes(x = y, y = rlo_pred), colour ="green")
#}
```


```{r define-is-groove, echo = F, warning = F, message = F}

hamby44$is_left_groove <- c(1, 1, 1, 1, 1, #5
                            1, 1, 1, 1, 0, #10
                            1, 1, 1, 1, 1, #15
                            1, 1, 1, 1, 1, #20
                            1, 1, 1, 1, 1, #25
                            1, 1, 1, 1, 1, #30
                            1, 0, 1, 1, 1, #35
                            1, 1, 1, 1, 1, #40
                            1, 1, 1, 1, 1, #45
                            1, 1, 1, 1, 1, #50
                            1, 1, 1, 1, 1, #55
                            1, 1, 0, 1, 1, #60
                            1, 1, 1, 1, 1, #65
                            1, 1, 1, 1, 1, #70
                            1, 1, 1, 1, 1, #75
                            1, 1, 1, 1, 1, #80
                            1, 1, 1, 1, 1, #85
                            1, 1, 1, 1, 1, #90
                            1, 1, 1, 1, 1, #95
                            1, 1, 1, 0, 1, #100
                            1, 1, 1, 1, 1, #105
                            1, 1, 1, 1, 1, #110
                            1, 1, 1, 1, 1, #115
                            1, 1, 1, 1, 1, #120
                            1, 1, 1, 1, 1, #125
                            1, 1, 1, 1, 1, #130
                            1, 1, 1, 1, 1, #135
                            1, 1, 1, 1, 1, #140
                            1, 1, 1, 1, 1, #145
                            1, 1, 1, 1, 1, #150
                            1, 1, 1, 1, 1, #155
                            1, 1, 1, 1, 1, #160
                            1, 1, 1, 1, 1, #165
                            1, 1, 1, 1, 1, #170
                            1, 1, 1, 1, 1, #175
                            1, 1, 1, 1, 1, #180
                            1, 1, 1, 1, 1, #185
                            1, 1, 1, 1, 1, #190
                            1, 1, 1, 1, 1, #195
                            1, 1, 1, 1, 1, #200
                            1, 1, 1, 1, 1, #205
                            1, 1, 1)

hamby44$is_right_groove <- c(1, 1, 0, 0, 0, #5 
                             1, 0, 0, 0, 1, #10
                             0, 0, 1, 1, 0, #15
                             0, 1, 1, 1, 0, #20
                             0, 0, 0, 1, 0, #25
                             1, 0, 0, 0, 0, #30
                             1, 1, 1, 0, 1, #35
                             0, 1, 1, 1, 0, #40
                             0, 0, 0, 1, 0, #45
                             1, 0, 1, 0, 0, #50
                             1, 0, 0, 1, 0, #55
                             0, 0, 1, 1, 0, #60
                             0, 1, 0, 0, 1, #65
                             1, 0, 1, 1, 1, #70
                             0, 0, 1, 0, 1, #75
                             0, 1, 0, 1, 0, #80
                             0, 1, 0, 1, 1, #85
                             1, 0, 1, 0, 1, #90
                             0, 0, 0, 1, 1, #95
                             1, 0, 1, 1, 1, #100
                             0, 0, 1, 1, 0, #105
                             0, 0, 1, 0, 0, #110
                             0, 1, 1, 1, 0, #115
                             0, 1, 0, 1, 0, #120
                             1, 1, 1, 0, 0, #125
                             0, 1, 1, 1, 1, #130
                             1, 1, 0, 1, 1, #135
                             1, 1, 0, 0, 0, #140
                             0, 0, 0, 0, 0, #145
                             0, 1, 1, 0, 0, #150
                             1, 1, 0, 1, 0, #155
                             0, 1, 0, 0, 0, #160
                             0, 0, 0, 0, 1, #165
                             1, 0, 1, 0, 0, #170
                             1, 1, 0, 0, 0, #175
                             0, 0, 1, 0, 1, #180
                             0, 1, 0, 1, 0, #185
                             1, 1, 1, 1, 0, #190
                             1, 1, 1, 0, 1, #195
                             1, 1, 1, 0, 1, #200
                             0, 1, 0, 1, 0, #205
                             1, 0, 1)


```


## Current features of the data  

Each land has been averaged across ten crosscuts, as well as shifted down so the lowest observed `value` is at 0; this column is referred to as `value_std`.  

For each land, the residuals from both a robust linear model (2nd order) and a robust LOESS model have been saved.  

## Additional feature creation  

We can define two additional columns, `depth` and `side`. `depth` represents the depth of each observed data point from the median observed `y` value. `side` represents whether the data point is to the left of the median or to the right of the median.  

We also need to define a response variable to work with; here, we will take the manually identified `grooves` value from the `hamby44` dataset, and classify anything outside of this range as a response: 1, and anything inside this range a response: 0. This is to indicate that if the response is 1, that data point lies in the groove engraved area.   


```{r define-depth-and-side, echo = F, warning = F, message = F}
hamby44 <- hamby44 %>% mutate(ccdata_w_resid = purrr::map(ccdata_w_resid, .f = function(bullet){
  median <- median(bullet$y)
  bullet$side <- "right"
  bullet$side <- ifelse(bullet$y <= median, "left", bullet$side)
  bullet$depth <- abs(bullet$y - median)
  return(bullet)
}))
#hamby44$ccdata_w_resid[[1]][3000,]
```

```{r define-response, echo = F, warning = F, message = F}

hamby44 <- hamby44 %>% mutate(left_groove = purrr::map_dbl(grooves, .f = function(grooves){
  left_groove <- grooves$groove[1]
}),
right_groove = purrr::map_dbl(grooves, .f = function(grooves){
  right_groove <- grooves$groove[2]
}))
#hamby44[1:6,]

calculate_response <- function(dataset){
  for(i in 1:nrow(dataset)){
  left_groove <- dataset$left_groove[i]
  right_groove <- dataset$right_groove[i]
  dataset$ccdata_w_resid[[i]]$left_groove <- left_groove
  dataset$ccdata_w_resid[[i]]$right_groove <- right_groove
  }
  return(dataset)
}

#tst <- calculate_response(hamby44[1:5,])
#head(tst)

hamby44 <- calculate_response(hamby44)

hamby44 <- hamby44 %>% mutate(ccdata_w_resid = purrr::map(ccdata_w_resid, .f = function(bullet){
  bullet$response <- ifelse(bullet$y <= bullet$left_groove | bullet$y >= bullet$right_groove, 1, 0)
  return(bullet)
}))

```


## Modeling  

Now we are going to use glmnet to do a logistic regression.  

```{r logistic-initial, echo = F, message = F, warning = F}
#library(plyr) ## should have been loaded earlier
#library(dplyr)

hamby44 <- hamby44 %>% mutate(logistic_fits = purrr::map(ccdata_w_resid, .f = function(bullet){
  bullet.model <- bullet[!is.na(bullet$rlo_resid),]
  glm0 <- glm(formula = response~rlo_resid + side + depth + side*depth, family = "binomial", data = bullet.model)
  bullet.model$pred_val <- predict(glm0, newdata = bullet.model, type = "response")
  bullet.model$pred_class <- ifelse(bullet.model$pred_val < .5, "LEA", "GEA")
  return(bullet.model)
}))

bullet.log <- rbind.fill(hamby44$ccdata_w_resid)
bullet.log <- bullet.log[!is.na(bullet.log$rlo_resid),]
glmA <- glm(formula = response~rlo_resid + side + depth + side*depth, family = "binomial", data = bullet.log)

summary(glmA)


preds.l <- predict(glmA, newdata = bullet.log, type = 'response')

# ROCR for ROC curve
library(ROCR)
# Calculate true positive rate and false positive rate on the prediction object
perf.l <- performance(prediction(preds.l, bullet.log$response), 'tpr', 'fpr')

plot(perf.l)

auc <- performance(prediction(preds.l, bullet.log$response), 'auc')@y.values[[1]]
auc


glmA_all <- coef(glmA)

hamby44 <- hamby44 %>% mutate(ccdata_log = purrr::map(ccdata_w_resid, .f = function(bullet){
  ## here is where we take model parameters and do stuff with them! 
  bullet <- bullet[!is.na(bullet$rlo_resid),]
  X <- model.matrix(~rlo_resid + side+ depth + side*depth, bullet)
  ymean <- X%*%glmA_all
  yhat <- exp(ymean)/(1 + exp(ymean))
  bullet$pred_val <- yhat
  bullet$pred_class <- ifelse(bullet$pred_val < .5, "LEA", "GEA")
  return(bullet)
}))

#head(hamby44$ccdata_logistic2[[1]])

plot_log_p <- function(bullet_num){
  bullet <- hamby44$ccdata_log[[bullet_num]]
  bullet %>% ggplot() + geom_point(aes(x = y, y = value_std, color = pred_class)) + theme_bw()
}

plp1 <- plot_log_p(1)
plp4 <- plot_log_p(4)
plp6 <- plot_log_p(6)
plp16 <- plot_log_p(16)

library(gridExtra)
grid.arrange(plp1, plp4, plp6, plp16, nrow = 2, top = "Predictions (cutoff: 0.5) using single logistic model for all data")


#cor(as.matrix(bullet.log[,-10]), use = "complete.obs") # nothing too terrible.. 
```

Traditional logistic regression (using `glm`) returns P-values equivalently 1 when dealing with a single bullet, which suggests we are overfitting with all the parameters included. Thus, we will use `glmnet` to do a ten-fold cross-validation of penalized logistic regression (LASSO).  

However, it is important to note that the model fit to all data simultaneously has values equivalently 0, but seems to do a fairly good job of predicting locations (see above image).  

First, we are going to fit an individual model to each of the bullet LEA's we have in the Hamby44 set, and average the parameter values from each of them (Dr. Hofmann's initial suggestion).  


```{r glmnet-stuff, echo = F, message = F, warning = F}
library(ROCR) # For ROC curves
library(glmnet) # For regularized GLMs
```

```{r glmnet-fits, warning = F, message = F}
hamby44 <- hamby44 %>% mutate(glmnet_fits = purrr::map(ccdata_w_resid, .f = function(bullet){
  bullet.model <- bullet[!is.na(bullet$rlo_resid),]
  X <- model.matrix( ~ rlo_resid + side + depth + side*depth - 1, bullet.model)

  # L1 regularized logistic regression
  fit <- cv.glmnet(x = X, y = bullet.model$response, family = 'binomial', type.measure = 'class', alpha = 1)
  return(fit)
}), matrix_fits = purrr::map(glmnet_fits, .f = function(fits){
  fits <- as.matrix(coef(fits))
}))

model_avg <- apply(simplify2array(hamby44$matrix_fits), 1, mean)
model_avg

```

Now, we want to use the averaged logistic regression parameters to fit the model to all of the bullets. 

Some examples of this are below:  

```{r, echo = F, warning = F, message = F}
hamby44 <- hamby44 %>% mutate(ccdata_logistic = purrr::map(ccdata_w_resid, .f = function(bullet){
  ## here is where we take model parameters and do stuff with them! 
  bullet <- bullet[!is.na(bullet$rlo_resid),]
  X <- cbind(1, model.matrix(~rlo_resid + side+ depth + side*depth - 1, bullet))
  ymean <- X%*%model_avg
  yhat <- exp(ymean)/(1 + exp(ymean))
  bullet$pred_val <- yhat
  bullet$pred_class <- ifelse(bullet$pred_val < .5, "LEA", "GEA")
  return(bullet)
}))

#head(hamby44$ccdata_logistic[[1]])

plot_log_pred <- function(bullet_num){
  bullet <- hamby44$ccdata_logistic[[bullet_num]]
  bullet %>% ggplot() + geom_point(aes(x = y, y = value_std, color = pred_class)) + theme_bw()
}

pl1 <- plot_log_pred(1)
pl4 <- plot_log_pred(4)
pl6 <- plot_log_pred(6)
pl16 <- plot_log_pred(16)

library(gridExtra)
grid.arrange(pl1, pl4, pl6, pl16, nrow = 2, top = "Predictions (cutoff: 0.5) using average of LASSO model parameters")
## It looks like this method is working pretty well with a cutoff of .5 for SOME bullets, but not for all bullets. 
## Could we possibly just put ALL the bullets together, and fit one model for all of them? Would that be any better?
#plot(fit)

## Alicia's suggestions: Try thinning the middle 50% of the data (grabbing every 3rd point or so), or try something more powerful, like random forest. 

```


Note that when we fit individual models, the ROC curves are essentially perfect on each bullet.  

Now let's try this combining all the data into one large data frame and fitting ONE logistic regression model to it... then we can see if the ROC curves are a little more reasonable.  

```{r, echo = F, warning = F, message = F}
bullet.model2 <- rbind.fill(hamby44$ccdata_w_resid)
bullet.model2 <- bullet.model2[!is.na(bullet.model2$rlo_resid),]


X <- model.matrix(~rlo_resid + side + depth + side*depth - 1, bullet.model2)
fit <- cv.glmnet(x = X, y = bullet.model2$response, family = 'binomial', type.measure = 'class', alpha = 1)
model_all <- as.matrix(coef(fit))

hamby44 <- hamby44 %>% mutate(ccdata_logistic2 = purrr::map(ccdata_w_resid, .f = function(bullet){
  ## here is where we take model parameters and do stuff with them! 
  bullet <- bullet[!is.na(bullet$rlo_resid),]
  X <- cbind(1, model.matrix(~rlo_resid + side+ depth + side*depth - 1, bullet))
  ymean <- X%*%model_all
  yhat <- exp(ymean)/(1 + exp(ymean))
  bullet$pred_val <- yhat
  bullet$pred_class <- ifelse(bullet$pred_val < .5, "LEA", "GEA")
  return(bullet)
}))

head(hamby44$ccdata_logistic2[[1]])

plot_log_pred2 <- function(bullet_num){
  bullet <- hamby44$ccdata_logistic2[[bullet_num]]
  bullet %>% ggplot() + geom_point(aes(x = y, y = value_std, color = pred_class)) + theme_bw()
}

pl1.2 <- plot_log_pred2(1)
pl4.2 <- plot_log_pred2(4)
pl6.2 <- plot_log_pred2(6)
pl16.2 <- plot_log_pred2(16)

library(gridExtra)
grid.arrange(pl1.2, pl4.2, pl6.2, pl16.2, nrow = 2, top = "Predictions (cutoff: 0.5) using single LASSO model for all data")
```



We can also look at the ROC curve from when we originally fit the data. 

```{r, echo = F, message = F, warning = F}

# Predict from model
preds <- predict(fit, newx = X, type = 'response')

# ROCR for ROC curve
# Calculate true positive rate and false positive rate on the prediction object
perf <- performance(prediction(preds, bullet.model2$response), 'tpr', 'fpr')

plot(perf)

auc <- performance(prediction(preds, bullet.model2$response), 'auc')@y.values[[1]]
auc
```

## Next steps:  

Going to hold out a training/testing set next and try that - current ROC/AUC aren't accurate representations of model performance on a hold-out set.  

*Note: The AUC reported above is most likely so high due to the lack of testing set hold-out AS WELL AS the fact that we are dealing with SO MANY data points - a small smattering of misidentifications - even if they are really important mistakes in our eyes - is still a very small percentage of the overall data points.*  

Additionally, might try thinning out middle 50% of data. We are dealing with an unbalanced response variable; there are way more 0 (LEA) values than 1 (GEA) values.  

Additional features to try:  
    - Quadratic term for robust LOESS residuals  
    - Standardizing robust LOESS residuals  
    - rlo_resid greater than cutoff value?  

If these don't initially work, I am suggesting we try random forest (Alicia suggested BART? seems like it could be overkill) or another two-class classification procedure to attack this. I do think the initial results are promising though!!  



## Plotting time t residuals vs. time t-1 residuals from robust LOESS  

```{r t-v-tmin1, echo = F, warning = F, message = F}
hamby44 <- hamby44 %>% mutate(ccdata_w_resid = purrr::map(ccdata_w_resid, .f = function(bullet){
  bullet$rlo_resid_lag1 <- c(0, bullet$rlo_resid[(1:(length(bullet$rlo_resid)-1))])
  return(bullet)
}))

resid_compare_plot <- function(bullet_num){
  bullet <- hamby44$ccdata_w_resid[[bullet_num]]
  bullet %>% ggplot() + geom_point(aes(x = rlo_resid, y = rlo_resid_lag1, colour = factor(response))) + theme_bw()
}


rcp1 <- resid_compare_plot(1)
rcp4 <- resid_compare_plot(4)
rcp6 <- resid_compare_plot(6)
rcp16 <- resid_compare_plot(16)

grid.arrange(rcp1, rcp4, rcp6, rcp16, nrow = 2, top = "Residuals vs. Lag 1 Residuals")
```


